{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "\n",
    "        albu.ShiftScaleRotate(scale_limit=2.0, rotate_limit=90, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n",
    "        albu.RandomCrop(height=320, width=320, always_apply=True),\n",
    "\n",
    "        albu.GaussNoise(p=0.2),\n",
    "        albu.Perspective(p=0.5),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.CLAHE(p=1),\n",
    "                albu.RandomBrightness(p=1),\n",
    "                albu.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.Sharpen(p=1),\n",
    "                albu.Blur(blur_limit=3, p=1),\n",
    "                albu.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.RandomContrast(p=1),\n",
    "                albu.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CLAHE transformation expects 1-channel or 3-channel images.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m original\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./augmented_images/\u001b[39m\u001b[39m{\u001b[39;00mimage_file\u001b[39m}\u001b[39;00m\u001b[39m_0.jpg\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m):  \u001b[39m# 증강된 이미지 10개 생성\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     augmented \u001b[39m=\u001b[39m augmentation(image\u001b[39m=\u001b[39;49mimage)\n\u001b[1;32m     15\u001b[0m     augmented_image \u001b[39m=\u001b[39m augmented[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     16\u001b[0m     result \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(augmented_image)\n",
      "File \u001b[0;32m~/miniconda3/envs/copd/lib/python3.9/site-packages/albumentations/core/composition.py:210\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    207\u001b[0m     p\u001b[39m.\u001b[39mpreprocess(data)\n\u001b[1;32m    209\u001b[0m \u001b[39mfor\u001b[39;00m idx, t \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(transforms):\n\u001b[0;32m--> 210\u001b[0m     data \u001b[39m=\u001b[39m t(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata)\n\u001b[1;32m    212\u001b[0m     \u001b[39mif\u001b[39;00m check_each_transform:\n\u001b[1;32m    213\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_data_post_transform(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/copd/lib/python3.9/site-packages/albumentations/core/composition.py:326\u001b[0m, in \u001b[0;36mOneOf.__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    324\u001b[0m     idx: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m random_utils\u001b[39m.\u001b[39mchoice(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms), p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms_ps)\n\u001b[1;32m    325\u001b[0m     t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms[idx]\n\u001b[0;32m--> 326\u001b[0m     data \u001b[39m=\u001b[39m t(force_apply\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata)\n\u001b[1;32m    327\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/copd/lib/python3.9/site-packages/albumentations/core/transforms_interface.py:118\u001b[0m, in \u001b[0;36mBasicTransform.__call__\u001b[0;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m             warn(\n\u001b[1;32m    114\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_class_fullname() \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m could work incorrectly in ReplayMode for other input data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m because its\u001b[39m\u001b[39m'\u001b[39m\u001b[39m params depend on targets.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m             )\n\u001b[1;32m    117\u001b[0m         kwargs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_key][\u001b[39mid\u001b[39m(\u001b[39mself\u001b[39m)] \u001b[39m=\u001b[39m deepcopy(params)\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_with_params(params, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    120\u001b[0m \u001b[39mreturn\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/copd/lib/python3.9/site-packages/albumentations/core/transforms_interface.py:131\u001b[0m, in \u001b[0;36mBasicTransform.apply_with_params\u001b[0;34m(self, params, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m     target_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_target_function(key)\n\u001b[1;32m    130\u001b[0m     target_dependencies \u001b[39m=\u001b[39m {k: kwargs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_dependence\u001b[39m.\u001b[39mget(key, [])}\n\u001b[0;32m--> 131\u001b[0m     res[key] \u001b[39m=\u001b[39m target_function(arg, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mdict\u001b[39;49m(params, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtarget_dependencies))\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     res[key] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/copd/lib/python3.9/site-packages/albumentations/augmentations/transforms.py:1417\u001b[0m, in \u001b[0;36mCLAHE.apply\u001b[0;34m(self, img, clip_limit, **params)\u001b[0m\n\u001b[1;32m   1415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, img, clip_limit\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[1;32m   1416\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_rgb_image(img) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_grayscale_image(img):\n\u001b[0;32m-> 1417\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCLAHE transformation expects 1-channel or 3-channel images.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1419\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mclahe(img, clip_limit, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtile_grid_size)\n",
      "\u001b[0;31mTypeError\u001b[0m: CLAHE transformation expects 1-channel or 3-channel images."
     ]
    }
   ],
   "source": [
    "augmentation = get_training_augmentation()\n",
    "\n",
    "for image_file in os.listdir('./images'):\n",
    "    if image_file.lower().endswith('.jpg'):\n",
    "        image = Image.open(os.path.join('./images', image_file))\n",
    "        image = image.resize((320, 320))\n",
    "        image = image.convert(\"RGB\")\n",
    "        image = np.array(image)\n",
    "        \n",
    "        original = Image.fromarray(image)\n",
    "        original.save(f\"./augmented_images/{image_file}_0.jpg\")\n",
    "        \n",
    "        for i in range(20):  # 증강된 이미지 10개 생성\n",
    "            augmented = augmentation(image=image)\n",
    "            augmented_image = augmented[\"image\"]\n",
    "            result = Image.fromarray(augmented_image)\n",
    "            result = result.convert(\"RGB\")\n",
    "            result.save(f\"./augmented_images/{image_file}_{i}.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "copd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
